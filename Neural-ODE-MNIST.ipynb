{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-aOU3hgZjEI"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch.nn  import functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtihiwPKZjEL"
      },
      "outputs": [],
      "source": [
        "def Euler_Method(z0, t0, t1, f):\n",
        "    h_max = 0.05\n",
        "    n_steps = math.ceil((abs(t1 - t0)/h_max).max().item())\n",
        "\n",
        "    h = (t1 - t0)/n_steps\n",
        "    t = t0\n",
        "    z = z0\n",
        "\n",
        "    for _ in range(n_steps):\n",
        "        z = z + h * f(z, t)\n",
        "        t = t + h\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYf3zKDjZjEM"
      },
      "outputs": [],
      "source": [
        "def Runge_Kutta(z0, t0, t1, f):\n",
        "    h_max = 0.05\n",
        "    n_steps = math.ceil((abs(t1 - t0)/h_max).max().item())\n",
        "\n",
        "    h = (t1 - t0)/n_steps\n",
        "    t = t0\n",
        "    z = z0\n",
        "\n",
        "    for _ in range(n_steps):\n",
        "        k1 = h * f(z, t)\n",
        "        k2 = h * f(z + k1/2, t + h/2)\n",
        "        k3 = h * f(z + k2/2, t + h/2)\n",
        "        k4 = h * f(z + k3, t + h)\n",
        "        z = z + (k1 + 2*k2 + 2*k3 + k4)/6\n",
        "        t = t + h\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCNxqQHKZjEM"
      },
      "outputs": [],
      "source": [
        "ode_solver = Runge_Kutta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOTugQk4ZjEN"
      },
      "outputs": [],
      "source": [
        "class ODEF(nn.Module):\n",
        "    def forward_with_grad(self, z, t, grad_outputs):\n",
        "        \"\"\"Compute f and a df/dz, a df/dp, a df/dt\"\"\"\n",
        "        batch_size = z.shape[0]\n",
        "\n",
        "        out = self.forward(z, t)\n",
        "\n",
        "        a = grad_outputs\n",
        "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
        "            (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a),\n",
        "            allow_unused=True, retain_graph=True\n",
        "        )\n",
        "        # grad method automatically sums gradients for batch items, we have to expand them back\n",
        "        if adfdp is not None:\n",
        "            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0)\n",
        "            adfdp = adfdp.expand(batch_size, -1) / batch_size\n",
        "        if adfdt is not None:\n",
        "            adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
        "        return out, adfdz, adfdt, adfdp\n",
        "\n",
        "    def flatten_parameters(self):\n",
        "        p_shapes = []\n",
        "        flat_parameters = []\n",
        "        for p in self.parameters():\n",
        "            p_shapes.append(p.size())\n",
        "            flat_parameters.append(p.flatten())\n",
        "        return torch.cat(flat_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1k_LIX-ZjEO"
      },
      "outputs": [],
      "source": [
        "class ODEAdjoint(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, z0, t, flat_parameters, func):\n",
        "        assert isinstance(func, ODEF)\n",
        "        bs, *z_shape = z0.size()\n",
        "        time_len = t.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            z = torch.zeros(time_len, bs, *z_shape).to(z0)\n",
        "            z[0] = z0\n",
        "            for i_t in range(time_len - 1):\n",
        "                z0 = ode_solver(z0, t[i_t], t[i_t+1], func)\n",
        "                z[i_t+1] = z0\n",
        "\n",
        "        ctx.func = func\n",
        "        ctx.save_for_backward(t, z.clone(), flat_parameters)\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dLdz):\n",
        "        \"\"\"\n",
        "        dLdz shape: time_len, batch_size, *z_shape\n",
        "        \"\"\"\n",
        "        func = ctx.func\n",
        "        t, z, flat_parameters = ctx.saved_tensors\n",
        "        time_len, bs, *z_shape = z.size()\n",
        "        n_dim = np.prod(z_shape)\n",
        "        n_params = flat_parameters.size(0)\n",
        "\n",
        "        # Dynamics of augmented system to be calculated backwards in time\n",
        "        def augmented_dynamics(aug_z_i, t_i):\n",
        "            \"\"\"\n",
        "            tensors here are temporal slices\n",
        "            t_i - is tensor with size: bs, 1\n",
        "            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1\n",
        "            \"\"\"\n",
        "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]  # ignore parameters and time\n",
        "\n",
        "            # Unflatten z and a\n",
        "            z_i = z_i.view(bs, *z_shape)\n",
        "            a = a.view(bs, *z_shape)\n",
        "            with torch.set_grad_enabled(True):\n",
        "                t_i = t_i.detach().requires_grad_(True)\n",
        "                z_i = z_i.detach().requires_grad_(True)\n",
        "                func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, t_i, grad_outputs=a)  # bs, *z_shape\n",
        "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n",
        "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n",
        "                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i)\n",
        "\n",
        "            # Flatten f and adfdz\n",
        "            func_eval = func_eval.view(bs, n_dim)\n",
        "            adfdz = adfdz.view(bs, n_dim)\n",
        "            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n",
        "\n",
        "        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n",
        "        with torch.no_grad():\n",
        "            ## Create placeholders for output gradients\n",
        "            # Prev computed backwards adjoints to be adjusted by direct gradients\n",
        "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
        "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
        "            # In contrast to z and p we need to return gradients for all times\n",
        "            adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n",
        "\n",
        "            for i_t in range(time_len-1, 0, -1):\n",
        "                z_i = z[i_t]\n",
        "                t_i = t[i_t]\n",
        "                f_i = func(z_i, t_i).view(bs, n_dim)\n",
        "\n",
        "                # Compute direct gradients\n",
        "                dLdz_i = dLdz[i_t]\n",
        "                dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
        "\n",
        "                # Adjusting adjoints with direct gradients\n",
        "                adj_z += dLdz_i\n",
        "                adj_t[i_t] = adj_t[i_t] - dLdt_i\n",
        "\n",
        "                # Pack augmented variable\n",
        "                aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim=-1)\n",
        "\n",
        "                # Solve augmented system backwards\n",
        "                aug_ans = ode_solver(aug_z, t_i, t[i_t-1], augmented_dynamics)\n",
        "\n",
        "                # Unpack solved backwards augmented system\n",
        "                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n",
        "                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n",
        "                adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:]\n",
        "\n",
        "                del aug_z, aug_ans\n",
        "\n",
        "            ## Adjust 0 time adjoint with direct gradients\n",
        "            # Compute direct gradients\n",
        "            dLdz_0 = dLdz[0]\n",
        "            dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
        "\n",
        "            # Adjust adjoints\n",
        "            adj_z += dLdz_0\n",
        "            adj_t[0] = adj_t[0] - dLdt_0\n",
        "        return adj_z.view(bs, *z_shape), adj_t, adj_p, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcuhxeltZjEO"
      },
      "outputs": [],
      "source": [
        "class NeuralODE(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super(NeuralODE, self).__init__()\n",
        "        assert isinstance(func, ODEF)\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n",
        "        t = t.to(z0)\n",
        "        z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func)\n",
        "        if return_whole_sequence:\n",
        "            return z\n",
        "        else:\n",
        "            return z[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wUdaHEbZjEO"
      },
      "outputs": [],
      "source": [
        "def norm(dim):\n",
        "    return nn.BatchNorm2d(dim)\n",
        "\n",
        "def conv3x3(in_feats, out_feats, stride=1):\n",
        "    return nn.Conv2d(in_feats, out_feats, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "def add_time(in_tensor, t):\n",
        "    bs, c, w, h = in_tensor.shape\n",
        "    return torch.cat((in_tensor, t.expand(bs, 1, w, h)), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQgBUauZZjEP"
      },
      "outputs": [],
      "source": [
        "class ConvODEF(ODEF):\n",
        "    def __init__(self, dim):\n",
        "        super(ConvODEF, self).__init__()\n",
        "        self.conv1 = conv3x3(dim + 1, dim)\n",
        "        self.norm1 = norm(dim)\n",
        "        self.conv2 = conv3x3(dim + 1, dim)\n",
        "        self.norm2 = norm(dim)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        xt = add_time(x, t)\n",
        "        h = self.norm1(torch.relu(self.conv1(xt)))\n",
        "        ht = add_time(h, t)\n",
        "        dxdt = self.norm2(torch.relu(self.conv2(ht)))\n",
        "        return dxdt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhAya_r3ZjEQ"
      },
      "outputs": [],
      "source": [
        "class ContinuousNeuralMNISTClassifier(nn.Module):\n",
        "    def __init__(self, ode):\n",
        "        super(ContinuousNeuralMNISTClassifier, self).__init__()\n",
        "        self.downsampling = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, 1),\n",
        "            norm(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),\n",
        "            norm(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),\n",
        "        )\n",
        "        self.feature = ode\n",
        "        self.norm = norm(64)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsampling(x)\n",
        "        x = self.feature(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.avg_pool(x)\n",
        "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
        "        x = x.view(-1, shape)\n",
        "        out = self.fc(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WTQTp90ZjEQ"
      },
      "outputs": [],
      "source": [
        "func = ConvODEF(64)\n",
        "ode = NeuralODE(func)\n",
        "model = ContinuousNeuralMNISTClassifier(ode)\n",
        "device = torch.device(\"cpu\") \n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYx8cyGQZjER",
        "outputId": "720cf43d-854c-4aec-ac1f-8c92262afd64"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "img_std = 0.3081\n",
        "img_mean = 0.1307\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(\"data/mnist\", train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                                 torchvision.transforms.ToTensor(),\n",
        "                                 torchvision.transforms.Normalize((img_mean,), (img_std,))\n",
        "                             ])\n",
        "    ),\n",
        "    batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(\"data/mnist\", train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                                 torchvision.transforms.ToTensor(),\n",
        "                                 torchvision.transforms.Normalize((img_mean,), (img_std,))\n",
        "                             ])\n",
        "    ),\n",
        "    batch_size=128, shuffle=True\n",
        ")\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf9KeZ3jZjER"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    num_items = 0\n",
        "    train_losses = []\n",
        "\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(f\"Training Epoch {epoch}...\")\n",
        "    for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses += [loss.item()]\n",
        "        num_items += data.shape[0]\n",
        "    print('Train loss: {:.5f}'.format(np.mean(train_losses)))\n",
        "    return train_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DapfRD5iZjER"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    accuracy = 0.0\n",
        "    num_items = 0\n",
        "\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(f\"Testing...\")\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in tqdm(enumerate(test_loader),  total=len(test_loader)):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            accuracy += torch.sum(torch.argmax(output, dim=1) == target).item()\n",
        "            num_items += data.shape[0]\n",
        "    accuracy = accuracy * 100 / num_items\n",
        "    print(\"Test Accuracy: {:.3f}%\".format(accuracy))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wxyQsIdZjES"
      },
      "outputs": [],
      "source": [
        "n_epochs = 3\n",
        "test()\n",
        "train_losses = []\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    train_losses += train(epoch)\n",
        "    test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1I41lEFMPz2",
        "outputId": "6e300bbb-652e-460e-c2f5-187ba7fa26dd"
      },
      "outputs": [],
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "pytorch_total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx3YHKlxOA1p",
        "outputId": "4755c098-f5bc-43ca-ffbb-97488c4a3d25"
      },
      "outputs": [],
      "source": [
        "pytorch_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "pytorch_train_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NShXeTfhqmY9"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QACTw6HqwHU"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 4\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self.make_layer(block, 8, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 16, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 32, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 64, num_blocks[3], stride=2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBqCGtR4q1Wf",
        "outputId": "91ac190d-9bbe-4156-9201-6bc28205250e"
      },
      "outputs": [],
      "source": [
        "model = ResNet18()\n",
        "device = torch.device(\"mps\")\n",
        "model.to(device)\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of parameters: \", total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laI29Ji8q-uf"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsJigGIZq_U4"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "for epoch in range(5):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDiW_HV3rDfn"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy: %.2f %%' % (100 * correct / total))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
